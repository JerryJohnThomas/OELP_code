{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e082526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a070583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6647fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "011cbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
    "from torch_geometric.nn import TransformerConv, TopKPooling \n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7f4b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d1afae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0\n",
      "Cuda available: True\n",
      "Torch geometric version: 2.0.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb90cb",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933b7f1",
   "metadata": {},
   "source": [
    "https://github.com/quarkslab/dataset-call-graph-blogpost-material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd4213",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/17JFlnMUjcsMmXQYXF8xZ3z169VDAwUls?authuser=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88786935",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=QLIkOtKS4os&list=PLV8yxwGOxvvoNkzPfCx2i8an--Tkt7O8Z&index=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0c3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = MoleculeDataset(root=\"data/\", filename=\"HIV_test.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35bff41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_temp= \"dataset-call-graph-blogpost-material/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39810b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data=pickle.load(open(path_temp+\"goodware_graphs.p\",\"rb\"))\n",
    "bad_data=pickle.load(open(path_temp+\"malware_graphs.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374db08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of array =  546\n",
      "length of 1 item =  2\n",
      "length of first thing in one item =  464\n",
      "length of second thing in one item =  464\n"
     ]
    }
   ],
   "source": [
    "print(\"length of array = \",len(good_data))\n",
    "print(\"length of 1 item = \",len(good_data[0]))\n",
    "print(\"length of first thing in one item = \",len(good_data[0][0]))\n",
    "print(\"length of second thing in one item = \",len(good_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a1fde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_menu=['mov', 'call', 'lea', 'jmp', 'push', 'add', 'xor', 'cmp', 'int3', 'nop', 'pushl', 'dec', 'sub', 'insl', 'inc','jz', 'jnz', 'je', 'jne', 'ja', 'jna', 'js', 'jns', 'jl', 'jnl', 'jg', 'jng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16078f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, root, filename,good_data, bad_data, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        self.good_data=good_data\n",
    "        self.bad_data=bad_data\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data_passed=self.good_data+self.bad_data\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return [\"goodware_graphs.p\",\"malware_graphs.p\"]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "#         self.data = self.data_passed\n",
    "        return \"NOt implemented yet\"\n",
    "\n",
    "        # if self.test:\n",
    "        #     return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        # else:\n",
    "        #     return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "#         self.data = self.data_passed\n",
    "        print(\"started doing stuff\")\n",
    "        index=0\n",
    "\n",
    "        for mol in (self.good_data):\n",
    "            \n",
    "\n",
    "            feature_data=mol[0]\n",
    "            neighbour_data=mol[1]\n",
    "\n",
    "            # i need to create inst ->index\n",
    "            instr_index=dict()\n",
    "            for a,b in enumerate(feature_data):\n",
    "              instr_index[b]=a \n",
    "\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(feature_data,instr_index)\n",
    "            # Get edge features\n",
    "            # edge_feats = self._get_edge_features(mol_obj)\n",
    "            edge_feats = []\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(neighbour_data,instr_index)\n",
    "\n",
    "            # Get labels info\n",
    "            label = 1\n",
    "\n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        smiles=mol\n",
    "                        ) \n",
    "                        \n",
    "            torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "\n",
    "            index+=1\n",
    "        \n",
    "        for mol in (self.bad_data):\n",
    "            \n",
    "\n",
    "            feature_data=mol[0]\n",
    "            neighbour_data=mol[1]\n",
    "\n",
    "            # i need to create inst ->index\n",
    "            instr_index=dict()\n",
    "            for a,b in enumerate(feature_data):\n",
    "              instr_index[b]=a \n",
    "\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(feature_data,instr_index)\n",
    "            # Get edge features\n",
    "            # edge_feats = self._get_edge_features(mol_obj)\n",
    "            edge_feats = []\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(neighbour_data,instr_index)\n",
    "\n",
    "            # Get labels info\n",
    "            label = 0\n",
    "\n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        smiles=mol\n",
    "                        ) \n",
    "                        \n",
    "            torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "\n",
    "            index+=1\n",
    "        \n",
    "        \n",
    "\n",
    "    def process_node_features(self,val, node_feature_menu):\n",
    "      # print(\"jj\")\n",
    "      # print(val)\n",
    "      node_out=[]\n",
    "      keys=val.keys()\n",
    "      for name in node_feature_menu : \n",
    "        if(name in keys):\n",
    "          node_out.append(val[name])\n",
    "        else:\n",
    "          node_out.append(0)\n",
    "      return node_out\n",
    "\n",
    "\n",
    "    def _get_node_features(self, node_features,instr_index):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        node_feature_menu=['mov', 'call', 'lea', 'jmp', 'push', 'add', 'xor', 'cmp', 'int3', 'nop', 'pushl', 'dec', 'sub', 'insl', 'inc','jz', 'jnz', 'je', 'jne', 'ja', 'jna', 'js', 'jns', 'jl', 'jnl', 'jg', 'jng']\n",
    "\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in node_features:\n",
    "            # print(\"atom is\",atom)\n",
    "            \n",
    "            node_feats = []\n",
    "            node_feats = self.process_node_features(node_features[atom],node_feature_menu)\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol, instr_index):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        all_edge_feats = []\n",
    "\n",
    "        # for bond in mol.GetBonds():\n",
    "        #     edge_feats = []\n",
    "        #     # Feature 1: Bond type (as double)\n",
    "        #     edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "        #     # Feature 2: Rings\n",
    "        #     edge_feats.append(bond.IsInRing())\n",
    "        #     # Append node features to matrix (twice, per direction)\n",
    "        #     all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        # all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def get_one_instr_adjacency(self,val,instr_index,self_instruction_number):\n",
    "        edge_val=[]\n",
    "        for a in val:\n",
    "            edge_val+=[[instr_index[self_instruction_number],instr_index[a]]]\n",
    "\n",
    "        return edge_val\n",
    "\n",
    "    def _get_adjacency_info(self, mol, instr_index):\n",
    "        \"\"\"\n",
    "        We could also use rdmolops.GetAdjacencyMatrix(mol)\n",
    "        but we want to be sure that the order of the indices\n",
    "        matches the order of the edge features\n",
    "        \"\"\"\n",
    "        edge_indices = []\n",
    "        for bond in mol:\n",
    "            edge_indices += self.get_one_instr_adjacency(mol[bond],instr_index,bond)\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "#         return self.data_pass.shape[0]\n",
    "        return len(self.data_passed)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        data = torch.load(os.path.join(self.processed_dir, \n",
    "                             f'data_{idx}.pt'))\n",
    "        return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fbe3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started doing stuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MoleculeDataset(root=\"data/\", filename=\"goodware_graphs.p\", good_data=good_data, bad_data=bad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cac9ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1222, 27], edge_index=[2, 3265], edge_attr=[0], y=0, smiles=[2])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a02df057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 437],\n",
      "        [  2,   4],\n",
      "        [  3, 412],\n",
      "        ...,\n",
      "        [491, 142],\n",
      "        [491, 289],\n",
      "        [491, 135]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1].edge_index.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b7d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [11.,  0.,  2.,  ...,  0.,  0.,  0.],\n",
      "        [ 7.,  4.,  2.,  ...,  0.,  0.,  0.],\n",
      "        [26., 10.,  0.,  ...,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1].x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c871475",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57c8edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size, model_params):\n",
    "        super(GNN, self).__init__()\n",
    "        embedding_size = model_params[\"model_embedding_size\"]\n",
    "        n_heads = model_params[\"model_attention_heads\"]\n",
    "        self.n_layers = model_params[\"model_layers\"]\n",
    "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
    "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
    "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
    "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
    "        edge_dim = model_params[\"model_edge_dim\"]\n",
    "\n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        self.pooling_layers = ModuleList([])\n",
    "        self.bn_layers = ModuleList([])\n",
    "\n",
    "        # Transformation layer\n",
    "        self.conv1 = TransformerConv(feature_size, \n",
    "                                    embedding_size, \n",
    "                                    heads=n_heads, \n",
    "                                    dropout=dropout_rate,\n",
    "                                    edge_dim=edge_dim,\n",
    "                                    beta=True) \n",
    "\n",
    "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
    "        self.bn1 = BatchNorm1d(embedding_size)\n",
    "\n",
    "        # Other layers\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(TransformerConv(embedding_size, \n",
    "                                                    embedding_size, \n",
    "                                                    heads=n_heads, \n",
    "                                                    dropout=dropout_rate,\n",
    "                                                    edge_dim=edge_dim,\n",
    "                                                    beta=True))\n",
    "\n",
    "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
    "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
    "            if i % self.top_k_every_n == 0:\n",
    "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
    "            \n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
    "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))  \n",
    "        self.linear3 = Linear(int(dense_neurons/2), 1)  \n",
    "\n",
    "    def forward(self, x, edge_attr, edge_index, batch_index):\n",
    "        # Initial transformation\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = torch.relu(self.transf1(x))\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        # Holds the intermediate graph representations\n",
    "        global_representation = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
    "            x = torch.relu(self.transf_layers[i](x))\n",
    "            x = self.bn_layers[i](x)\n",
    "            # Always aggregate last layer\n",
    "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
    "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](\n",
    "                    x, edge_index, edge_attr, batch_index\n",
    "                    )\n",
    "                # Add current representation\n",
    "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
    "    \n",
    "        x = sum(global_representation)\n",
    "\n",
    "        # Output block\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd167613",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48778183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
    "    # Enumerate over the data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), \n",
    "                                batch.edge_attr.float(),\n",
    "                                batch.edge_index, \n",
    "                                batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    return running_loss/step\n",
    "\n",
    "def test(epoch, model, test_loader, loss_fn):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)  \n",
    "        pred = model(batch.x.float(), \n",
    "                        batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "\n",
    "         # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    print(all_preds_raw[0][:10])\n",
    "    print(all_preds[:10])\n",
    "    print(all_labels[:10])\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
    "    log_conf_matrix(all_preds, all_labels, epoch)\n",
    "    return running_loss/step\n",
    "\n",
    "def log_conf_matrix(y_pred, y_true, epoch):\n",
    "    # Log confusion matrix as image\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    classes = [\"0\", \"1\"]\n",
    "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
    "#     cfm_plot.figure.savefig(f'data/images/cm_{epoch}.png')\n",
    "#     mlflow.log_artifact(f\"data/images/cm_{epoch}.png\")\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_pred, y_true)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_pred, y_true)}\")\n",
    "    prec = precision_score(y_pred, y_true)\n",
    "    rec = recall_score(y_pred, y_true)\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    mlflow.log_metric(key=f\"Precision-{type}\", value=float(prec), step=epoch)\n",
    "    mlflow.log_metric(key=f\"Recall-{type}\", value=float(rec), step=epoch)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_pred, y_true)\n",
    "        print(f\"ROC AUC: {roc}\")\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    except:\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
    "        print(f\"ROC AUC: notdefined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
